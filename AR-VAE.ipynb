{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7d119e-1af4-4404-8f7a-c83755821ab6",
   "metadata": {},
   "source": [
    "# AR-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b62062-62ba-435b-8b4e-127aed3c5eef",
   "metadata": {},
   "source": [
    "We consider AR-VAE, a variational autoencoder in which the prior is parameterized by an autoregressive model rather than a standard Gaussian. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0466a-eeb3-4c9a-8eee-771bf8b62893",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abd2017-c6da-4d43-92f6-aeccdb7ecf3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T16:40:02.085003Z",
     "iopub.status.busy": "2023-11-08T16:40:02.084737Z",
     "iopub.status.idle": "2023-11-08T16:40:04.790593Z",
     "shell.execute_reply": "2023-11-08T16:40:04.789970Z",
     "shell.execute_reply.started": "2023-11-08T16:40:02.084978Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "\n",
    "!pip -q install datasets pychalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5681a5-d5e2-40e4-82ec-17932b43ea35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T16:40:06.888712Z",
     "iopub.status.busy": "2023-11-08T16:40:06.888257Z",
     "iopub.status.idle": "2023-11-08T16:40:08.151954Z",
     "shell.execute_reply": "2023-11-08T16:40:08.151311Z",
     "shell.execute_reply.started": "2023-11-08T16:40:06.888679Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9540156-f347-4fda-a9b3-9c25da42c968",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5819cbb8-35c9-45f7-b38e-913f189f33d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T13:09:43.409349Z",
     "iopub.status.busy": "2023-11-08T13:09:43.408965Z",
     "iopub.status.idle": "2023-11-08T13:09:43.416685Z",
     "shell.execute_reply": "2023-11-08T13:09:43.415973Z",
     "shell.execute_reply.started": "2023-11-08T13:09:43.409315Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "\n",
    "class CelebAFaces(Dataset):\n",
    "    \"\"\"CelebA faces dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    resolution: int - Image resolution.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    \n",
    "    >>> dataset = CelebAFaces(resolution=256)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, resolution: int) -> None:\n",
    "        \"\"\"Initializes the dataset.\"\"\"\n",
    "        \n",
    "        self.resolution = resolution\n",
    "        \n",
    "        # Load the dataset in streaming mode from Hugging Face.\n",
    "        \n",
    "        self._dataset = load_dataset(\n",
    "            path='nielsr/CelebA-faces', \n",
    "            split='train', \n",
    "            streaming=True,\n",
    "        ).shuffle()\n",
    "        \n",
    "        # Create an iterator for the dataset.\n",
    "        \n",
    "        self._dataset_iterator = iter(self._dataset)\n",
    "        \n",
    "        # Define a transform to be applied to each example.\n",
    "        \n",
    "        self._transform = transforms.Compose([\n",
    "            transforms.Resize(\n",
    "                size=(self.resolution, self.resolution),\n",
    "                interpolation=0,\n",
    "            ),\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.Lambda(lambda x: x / 255.),\n",
    "        ])\n",
    "        \n",
    "        # Get the length of the dataset.\n",
    "        \n",
    "        self._length = self._dataset.info.splits['train'].num_examples\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the dataset.\"\"\"\n",
    "        \n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        \"\"\"Returns an example from the dataset.\"\"\"\n",
    "        \n",
    "        example = next(self._dataset_iterator)['image']\n",
    "        example = self._transform(example)\n",
    "        \n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c40c0aa8-66f2-4af1-a76c-ec7ab38dd64a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T13:09:48.888425Z",
     "iopub.status.busy": "2023-11-08T13:09:48.888030Z",
     "iopub.status.idle": "2023-11-08T13:09:49.327104Z",
     "shell.execute_reply": "2023-11-08T13:09:49.326526Z",
     "shell.execute_reply.started": "2023-11-08T13:09:48.888398Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "\n",
    "train_dataset = CelebAFaces(resolution=128)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13023b0-7e72-4c23-9f67-45839b43f78f",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09e9052-853f-4139-996b-f5774d3440c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T16:46:59.721909Z",
     "iopub.status.busy": "2023-11-08T16:46:59.721417Z",
     "iopub.status.idle": "2023-11-08T16:46:59.726461Z",
     "shell.execute_reply": "2023-11-08T16:46:59.725743Z",
     "shell.execute_reply.started": "2023-11-08T16:46:59.721880Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "\n",
    "@dataclass\n",
    "class CausalCNNOptions:\n",
    "    \"\"\"Causal CNN options.\"\"\"\n",
    "\n",
    "    hidden_channels: int\n",
    "    kernel_size: int\n",
    "    vocabulary_size: int\n",
    "    sequence_length: int\n",
    "    embedding_dimension: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "f4299250-fb49-4b4b-88a2-eec5e641285c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T18:53:05.140996Z",
     "iopub.status.busy": "2023-11-08T18:53:05.140593Z",
     "iopub.status.idle": "2023-11-08T18:53:05.153423Z",
     "shell.execute_reply": "2023-11-08T18:53:05.152775Z",
     "shell.execute_reply.started": "2023-11-08T18:53:05.140967Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "\n",
    "class CausalCNN(nn.Module):\n",
    "    \"\"\"Causal CNN.\"\"\"\n",
    "    \n",
    "    def __init__(self, options: CausalCNNOptions) -> None:\n",
    "        \"\"\"Initializes the module.\"\"\"\n",
    "        \n",
    "        super(CausalCNN, self).__init__()\n",
    "        self.options = options\n",
    "        \n",
    "        self.position_embedding = nn.Embedding(num_embeddings=options.sequence_length, embedding_dim=options.embedding_dimension)\n",
    "        \n",
    "        self.convolution_1 = nn.Conv1d(in_channels=options.embedding_dimension + options.vocabulary_size, out_channels=options.hidden_channels, kernel_size=options.kernel_size, stride=1, padding=0, dilation=1)\n",
    "        self.convolution_2 = nn.Conv1d(in_channels=options.hidden_channels, out_channels=options.hidden_channels, kernel_size=options.kernel_size, stride=1, padding=0, dilation=2)\n",
    "        self.convolution_3 = nn.Conv1d(in_channels=options.hidden_channels, out_channels=options.hidden_channels, kernel_size=options.kernel_size, stride=1, padding=0, dilation=4)\n",
    "        self.convolution_4 = nn.Conv1d(in_channels=options.hidden_channels, out_channels=options.hidden_channels, kernel_size=options.kernel_size, stride=1, padding=0, dilation=16)\n",
    "        self.convolution_5 = nn.Conv1d(in_channels=options.hidden_channels, out_channels=options.hidden_channels, kernel_size=options.kernel_size, stride=1, padding=0, dilation=64)\n",
    "        self.convolution_6 = nn.Conv1d(in_channels=options.hidden_channels, out_channels=options.vocabulary_size, kernel_size=options.kernel_size, stride=1, padding=0, dilation=128)\n",
    "\n",
    "        self.padding_1 = (self.options.kernel_size, -1)\n",
    "        self.padding_2 = ((self.options.kernel_size - 1) * 2, 0, 0, 0)\n",
    "        self.padding_3 = ((self.options.kernel_size - 1) * 4, 0, 0, 0)\n",
    "        self.padding_4 = ((self.options.kernel_size - 1) * 16, 0, 0, 0)\n",
    "        self.padding_5 = ((self.options.kernel_size - 1) * 64, 0, 0, 0)\n",
    "        self.padding_6 = ((self.options.kernel_size - 1) * 128, 0, 0, 0)\n",
    "        \n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        \n",
    "        # Embeddings.\n",
    " \n",
    "        tokens = tokens.to(int).cuda()\n",
    "        positions = torch.arange(self.options.sequence_length).to(int).cuda()\n",
    "        positions = positions.view(1, -1).repeat(tokens.size(0), 1)\n",
    "        \n",
    "        token_embeddings = F.one_hot(tokens, num_classes=self.options.vocabulary_size)\n",
    "        position_embeddings = self.position_embedding(positions)\n",
    "        \n",
    "        x = torch.cat((token_embeddings, position_embeddings), dim=-1)\n",
    "        \n",
    "        # CNN.\n",
    "        \n",
    "        x = x.transpose(-2, -1)\n",
    "        x = F.leaky_relu(self.convolution_1(F.pad(x, self.padding_1)))\n",
    "        x = F.leaky_relu(self.convolution_2(F.pad(x, self.padding_2))) #+ x\n",
    "        x = F.leaky_relu(self.convolution_3(F.pad(x, self.padding_3))) #+ x\n",
    "        x = F.leaky_relu(self.convolution_4(F.pad(x, self.padding_4))) #+ x\n",
    "        x = F.leaky_relu(self.convolution_5(F.pad(x, self.padding_5))) #+ x\n",
    "        x = self.convolution_6(F.pad(x, self.padding_6))\n",
    "        x = x.transpose(-2, -1)\n",
    "        \n",
    "        # Logits.\n",
    "        \n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "dec08076-6051-4c3f-852e-8d1585ad11a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:00:37.491200Z",
     "iopub.status.busy": "2023-11-08T19:00:37.490816Z",
     "iopub.status.idle": "2023-11-08T19:00:37.496242Z",
     "shell.execute_reply": "2023-11-08T19:00:37.495560Z",
     "shell.execute_reply.started": "2023-11-08T19:00:37.491174Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27458"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "ae0618a4-199a-4bc6-81a6-8ce06668198c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T18:53:23.692199Z",
     "iopub.status.busy": "2023-11-08T18:53:23.691811Z",
     "iopub.status.idle": "2023-11-08T18:53:23.763534Z",
     "shell.execute_reply": "2023-11-08T18:53:23.762801Z",
     "shell.execute_reply.started": "2023-11-08T18:53:23.692172Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "mnist_dataset = MNIST(root='.', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.round()),\n",
    "]))\n",
    "\n",
    "mnist_dataloader = DataLoader(dataset=mnist_dataset, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "bb6cb204-b1c6-42fb-8a74-da43b3c974ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:10:17.451183Z",
     "iopub.status.busy": "2023-11-08T19:10:17.450787Z",
     "iopub.status.idle": "2023-11-08T19:10:17.455628Z",
     "shell.execute_reply": "2023-11-08T19:10:17.454719Z",
     "shell.execute_reply.started": "2023-11-08T19:10:17.451157Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "\n",
    "#model = CausalCNN(CausalCNNOptions(hidden_channels=32, kernel_size=6, sequence_length=28*28, vocabulary_size=2, embedding_dimension=2)).cuda()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "93deac62-d6ff-4970-bf77-99d7688a850b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:10:19.913372Z",
     "iopub.status.busy": "2023-11-08T19:10:19.912960Z",
     "iopub.status.idle": "2023-11-08T19:11:33.441393Z",
     "shell.execute_reply": "2023-11-08T19:11:33.440441Z",
     "shell.execute_reply.started": "2023-11-08T19:10:19.913346Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/5, batch 1/468 - 0.07550138235092163\n",
      "epoch 1/5, batch 101/468 - 0.07674923539161682\n",
      "epoch 1/5, batch 201/468 - 0.07591504603624344\n",
      "epoch 1/5, batch 301/468 - 0.07670727372169495\n",
      "epoch 1/5, batch 401/468 - 0.07553686946630478\n",
      "epoch 2/5, batch 1/468 - 0.07472698390483856\n",
      "epoch 2/5, batch 101/468 - 0.07491572201251984\n",
      "epoch 2/5, batch 201/468 - 0.07731043547391891\n",
      "epoch 2/5, batch 301/468 - 0.07579892873764038\n",
      "epoch 2/5, batch 401/468 - 0.07346445322036743\n",
      "epoch 3/5, batch 1/468 - 0.07726892828941345\n",
      "epoch 3/5, batch 101/468 - 0.07493554800748825\n",
      "epoch 3/5, batch 201/468 - 0.07357580214738846\n",
      "epoch 3/5, batch 301/468 - 0.07335098087787628\n",
      "epoch 3/5, batch 401/468 - 0.07823026180267334\n",
      "epoch 4/5, batch 1/468 - 0.07491743564605713\n",
      "epoch 4/5, batch 101/468 - 0.07583694905042648\n",
      "epoch 4/5, batch 201/468 - 0.07581239193677902\n",
      "epoch 4/5, batch 301/468 - 0.07405684143304825\n",
      "epoch 4/5, batch 401/468 - 0.07885908335447311\n",
      "epoch 5/5, batch 1/468 - 0.0764915943145752\n",
      "epoch 5/5, batch 101/468 - 0.07200847566127777\n",
      "epoch 5/5, batch 201/468 - 0.07869178801774979\n",
      "epoch 5/5, batch 301/468 - 0.07548265159130096\n",
      "epoch 5/5, batch 401/468 - 0.07601013779640198\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_index, (y, _) in enumerate(mnist_dataloader):\n",
    "        if (len(y) != 128):\n",
    "            continue\n",
    "            \n",
    "        y = y.to(int).cuda().view(128, 28*28)\n",
    "        yp = model(y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(yp.view(-1, 2), y.flatten())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_index % 100) == 0:\n",
    "            print(f'epoch {epoch + 1}/{epochs}, batch {batch_index + 1}/{len(mnist_dataset)//128} - {loss.detach().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "id": "42618ee3-2a29-4262-a926-473689fb3296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:23:12.786133Z",
     "iopub.status.busy": "2023-11-08T19:23:12.785752Z",
     "iopub.status.idle": "2023-11-08T19:23:12.792345Z",
     "shell.execute_reply": "2023-11-08T19:23:12.791553Z",
     "shell.execute_reply.started": "2023-11-08T19:23:12.786106Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_greedy(model: CausalCNN, prefix: torch.Tensor = None) -> torch.Tensor:\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    sequence_length = model.options.sequence_length\n",
    "    offset = 0\n",
    "    sample = torch.zeros(sequence_length).cuda()\n",
    "    \n",
    "    if prefix is not None:\n",
    "        offset = prefix.size(0)\n",
    "        sample[: offset] = prefix\n",
    "    \n",
    "    for i in range(offset, sequence_length):\n",
    "        logits = model(sample.view(1, sequence_length))[0, i].detach()\n",
    "        probability = torch.exp(logits)[1]\n",
    "        value = random.random() < probability\n",
    "        sample[i] = value\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "93cf7ae5-d507-4b70-9a5d-911c66bae856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:25:15.540398Z",
     "iopub.status.busy": "2023-11-08T19:25:15.540021Z",
     "iopub.status.idle": "2023-11-08T19:25:15.544492Z",
     "shell.execute_reply": "2023-11-08T19:25:15.543705Z",
     "shell.execute_reply.started": "2023-11-08T19:25:15.540372Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_prefix(model, prefix):\n",
    "    \n",
    "    sequence_length = model.options.sequence_length\n",
    "    offset = prefix.size(0)\n",
    "\n",
    "    sample = torch.zeros(sequence_length).cuda()\n",
    "    sample[: offset] = prefix\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "id": "ba21aec5-09a4-4911-a54b-ec36d8874893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:41:33.876569Z",
     "iopub.status.busy": "2023-11-08T19:41:33.876198Z",
     "iopub.status.idle": "2023-11-08T19:41:33.881511Z",
     "shell.execute_reply": "2023-11-08T19:41:33.880794Z",
     "shell.execute_reply.started": "2023-11-08T19:41:33.876541Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_distribution(model, prefix):\n",
    "    model.eval()\n",
    "        \n",
    "    sequence_length = model.options.sequence_length\n",
    "    offset = 0\n",
    "    sample = torch.zeros(sequence_length).cuda()\n",
    "    \n",
    "    if prefix is not None:\n",
    "        offset = prefix.size(0)\n",
    "        sample[: offset] = prefix\n",
    "    \n",
    "    logits = model(sample.view(1, sequence_length)).detach()\n",
    "    probability = torch.exp(logits)[0,:,1]\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "id": "b0e01114-0a33-4dc8-8f56-c3009d75311c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:47:10.631672Z",
     "iopub.status.busy": "2023-11-08T19:47:10.631287Z",
     "iopub.status.idle": "2023-11-08T19:47:10.637715Z",
     "shell.execute_reply": "2023-11-08T19:47:10.637078Z",
     "shell.execute_reply.started": "2023-11-08T19:47:10.631648Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAAAAACIM/FCAAABP0lEQVR4nO3b0UrEMBRF0VHm/395fJcSGk2Tw2GtxxGcbC5cS21fLwAAAAAAAAAADvs6fYBrn9EPL8/8/cxB9hOSRkiampD36QOMN+19NRMRkkZImpqQnVe/84t24nQ1ExGSRkiampAt63f+VsK8mokISSMkTU3Is+v3cu8+85U1ExGSRkgaIWl23sR+9G9WzUSEpBGSpibkofW76B+DE2omIiSNkDQ1IauvSEf3TT5XH65SMxEhaYSkqQlZdfX7a+3ufy66ZiJC0ghJUxOy+ubDsfdRaiYiJI2QNDUhq9bv8dcAayYiJI2QNELSCEkjJI2QNDUhB16E8ST2kJA0QtLUhNzahRPrc/TonAeY7xCSRkiampA/rN/Vv32NmokISSMkTU3IzIK8s4U9wvFfQtIISVMTAgAAAAAAAADAcT9tlA7pjYfmIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=200x200>"
      ]
     },
     "execution_count": 1192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = mnist_dataset[35][0]\n",
    "\n",
    "transforms.ToPILImage()(example).resize((200, 200), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "id": "1b5f43f4-8060-4fa9-b09f-cb79ad1b8e5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:47:14.071948Z",
     "iopub.status.busy": "2023-11-08T19:47:14.071097Z",
     "iopub.status.idle": "2023-11-08T19:47:14.078498Z",
     "shell.execute_reply": "2023-11-08T19:47:14.077710Z",
     "shell.execute_reply.started": "2023-11-08T19:47:14.071909Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAAAAACIM/FCAAAAnElEQVR4nO3YQQqEMBBEUR3m/ld2LhBkskhSFO9tXdifhka8LgAAAAAAAAAADrtPDzD2vD0czvxZM8h+QtIISVMT8j09wPul/V/NRoSkEZKmJmTn1+/8oZ2YrmYjQtIISVMTsuX8zv9KmFezESFphKSpCVl7fod3d80razYiJI2QNEIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKDSD2LRBUYTVINFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=200x200>"
      ]
     },
     "execution_count": 1193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = example.flatten()[: 300]\n",
    "\n",
    "transforms.ToPILImage()(show_prefix(model, prefix).view(28, 28)).resize((200, 200), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "id": "22f6ebce-8ae8-497a-bfa4-d808f4dfbbd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:47:17.992401Z",
     "iopub.status.busy": "2023-11-08T19:47:17.991989Z",
     "iopub.status.idle": "2023-11-08T19:47:18.000728Z",
     "shell.execute_reply": "2023-11-08T19:47:18.000096Z",
     "shell.execute_reply.started": "2023-11-08T19:47:17.992374Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAAAAACIM/FCAAAEEklEQVR4nO2dO2tUURSF52V8RuIDNQgqaq2V/0Gws7Cwt7G38jf4I/RPWNuLYKmFiApR0MQgicY4icVeu1iHzXBnJsWenfU1i3PnPrJyYM3Ovufe9HpCCCGEEEIcFvpZTrs/5xUHcx6fBhnJhoxko4yR0QGdpz9xGG2cmLfTh3GZGZGRbMhINmQkGzOU8f1oNODhkLaGXyB74ZCFd5lImRmRkWzISDbKGJkmfvuRDEj8zwLO3SGdpknYsQki9h+NOJonpnCZGZGRbMhINsoY6dRFmRS4TcIO6LQj2gfxOSZx9mhX8NbkpckXbF03eUG7lpkRGcmGjGSjjJFpmthci3Km7vM+YdmL0UmTG9h6yeS5Cf9iUQtfNPmArQ+iH67MjMhINmQkG2WMzBC/ANVrXP0eM0G0njW5bfKM9mxOt2Hy3WTN5J7Jb+zKjWG+8OIjI9mQkWyUMdIpfrmk5R4E32Tz4arJI5O7JtdNkKKfcMRHk/smf02azkQHysyIjGRDRrJRxsgM1W+4VGGJd0XZe9Pkmslrkycmr6a4cgfKzIiMZENGsiEj2Zh7JTa+R1B+9+6YPDW5YvLYBPX/GxM0WppuzC6Nwn5JTJkZkZFsyEg2yhjx+A1XJXvFPogEoH73tsdxk4cmR0zW6GzNXUe+1JBP150yMyIj2ZCRbJQxEi9g7rNCENVHTRCUiF/0S3rvTX7S6fDbCtd+NNVvl7XKIWVmREayISPZKGNk8vMjCNwlkgsmiF8stvC8RBMCEc3RukWfed2NW4r84AiPulBmRmQkGzKSjTJGmjo3HHlHgVN42wR5u8xHIHfRfOAW7gkThLF3e7fpQKD4XWBkJBsyko04ft0eB254k4yf0PMjsSsq5NN0mk0TX47MT0/7LTza2IUyMyIj2ZCRbJQx4i0ADrrYHrciELjISy9wUe+ieXDV5BRdA2WvP3EypiNCmmI8DOUyMyIj2ZCRbMhINpoVdAh3X912xgRtZyyvRr/js8lOeIIVGgF85Zw3+YGt3Dfh0V60MabMjMhINmQkG2WMjHiVBqrwy/gU8XfL5JwJcnOLDkRPxZ/4Rtt7mfbZoCv/gnJRjsBuQr0DZWZERrIhI9koY2QUtk++YYj2x1cTZCJeAbdiguNRIHu/Ggs7Vmk0ppN7SYvARRclfBWd3sC8iMhINmQkG01/GL6aMEUmIiHRxEYtvEMH+pGbNMI+aD7g5mGTqbjGHxM9P7L4yEg2ZCQb8QLmISsaxGglwDvid8XE8/KdyS5t5RuLY9rFmfnBkR79VIuPjGRDRrJRxkin/z8yJMHzI7gDt08jh991z++wi9/ZPC9lZkRGsiEj2ShjZIZ//9QFDtzwswOmzIzISDZkJBsyIoQQQgghhBBCCHGo+A/ETbRe21z38AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=200x200>"
      ]
     },
     "execution_count": 1194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToPILImage()(show_distribution(model, prefix).view(28, 28)).resize((200,200),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "id": "68730469-7abf-4958-abf5-846b3342a096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:47:20.218498Z",
     "iopub.status.busy": "2023-11-08T19:47:20.217830Z",
     "iopub.status.idle": "2023-11-08T19:47:20.738907Z",
     "shell.execute_reply": "2023-11-08T19:47:20.738067Z",
     "shell.execute_reply.started": "2023-11-08T19:47:20.218471Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAAAAACIM/FCAAABO0lEQVR4nO3cwWrCUBBAUS39/19uNy00EKSRvOQynLPUhe8yMEqiPh4AAAAAAAAAANzsefcB9n29enL3zB9rDnI9ITVCasaEfN59gNeb9v/GTERIjZCaMSFXfvo9vmgPnG7MRITUCKkZE3LJ+j1+KeG4MRMRUiOkZkzI2vW7u3fXvOSYiQipEVIjpObspX7hO8fWmIkIqRFSMybkrM24f6HkwkvkYyYipEZIjZAaITVCaoTUrP30u/Qlt8ZMREiNkJoxIYvW73P30ZUnGDMRITVCasaE3Pfzvd/VbP1uCakRUiOkRkiNkBohNUJqhNQIqRFSI6RGSI2QmjEhZ/8J0s+l6QP3EE8yZiJCaoTUjAk5cgdvzTJ1D3FLSI2QmjEhbyy/t7fw0q+LjJmIkBohNWNCAAAAAAAAAP76Bht0D+q0ogIlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=200x200>"
      ]
     },
     "execution_count": 1195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToPILImage()(sample_greedy(model, prefix=prefix).view(28, 28)).resize((200, 200), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "id": "f870c953-bf9d-4dd1-99bf-6820747f9674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:36:07.947749Z",
     "iopub.status.busy": "2023-11-08T19:36:07.947368Z",
     "iopub.status.idle": "2023-11-08T19:36:07.953896Z",
     "shell.execute_reply": "2023-11-08T19:36:07.953114Z",
     "shell.execute_reply.started": "2023-11-08T19:36:07.947723Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'causal-cnn-mnist-2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "51048f67-36e0-4553-96dc-f20b9e338352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T19:02:09.405591Z",
     "iopub.status.busy": "2023-11-08T19:02:09.405186Z",
     "iopub.status.idle": "2023-11-08T19:02:09.418527Z",
     "shell.execute_reply": "2023-11-08T19:02:09.417552Z",
     "shell.execute_reply.started": "2023-11-08T19:02:09.405562Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1031551726.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[726], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    class CausalCNN\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#@markdown\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CausalCNN\n",
    "    \n",
    "\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "  \"\"\"A convolutional variational autoencoder.\"\"\"\n",
    "\n",
    "  def __init__(self, in_channels: int = 3) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.GroupNorm(num_channels=64, num_groups=8),\n",
    "        nn.LeakyReLU(0.2),\n",
    "\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.GroupNorm(num_channels=64, num_groups=8),\n",
    "        nn.LeakyReLU(0.2),\n",
    "\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.GroupNorm(num_channels=64, num_groups=8),\n",
    "        nn.LeakyReLU(0.2),\n",
    "\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.GroupNorm(num_channels=64, num_groups=8),\n",
    "        nn.LeakyReLU(0.2),\n",
    "\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.GroupNorm(num_channels=64, num_groups=8),\n",
    "        nn.LeakyReLU(0.2),\n",
    "\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.GroupNorm(num_channels=64, num_groups=8),\n",
    "        nn.LeakyReLU(0.2),\n",
    "\n",
    "        nn.Flatten(),\n",
    "    )\n",
    "\n",
    "    latent_size = 256\n",
    "    flatten_size = 64 * 2 * 2\n",
    "    unflatten_size = (64, 2, 2)\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "\n",
    "        nn.Linear(latent_size, flatten_size),\n",
    "        nn.Unflatten(-1, unflatten_size),\n",
    "        nn.LeakyReLU(),\n",
    "\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(num_features=64),\n",
    "        nn.LeakyReLU(),\n",
    "\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(num_features=64),\n",
    "        nn.LeakyReLU(),\n",
    "\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(num_features=64),\n",
    "        nn.LeakyReLU(),\n",
    "\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=in_channels, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(num_features=in_channels),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "    self.encoder_mean = nn.Linear(flatten_size, latent_size)\n",
    "    self.encoder_log_variance = nn.Linear(flatten_size, latent_size)\n",
    "    \n",
    "    \n",
    "  def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Maps observations to Gaussian distributions over latents.\"\"\"\n",
    "\n",
    "    x = self.encoder(x)\n",
    "    z_mean = self.encoder_mean(x)\n",
    "    z_log_variance = self.encoder_log_variance(x)\n",
    "\n",
    "    return z_mean, z_log_variance\n",
    "\n",
    "  def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Maps latents to Gaussian distributions over observations.\"\"\"\n",
    "\n",
    "    x = self.decoder(z)\n",
    "\n",
    "    return x\n",
    "\n",
    "  def sample(\n",
    "      self,\n",
    "      z_mean: torch.Tensor,\n",
    "      z_log_variance: torch.Tensor,\n",
    "  ) -> torch.Tensor:\n",
    "    \"\"\"Samples from Gaussian distributions over latents.\"\"\"\n",
    "\n",
    "    epsilon = torch.randn_like(z_mean)\n",
    "    z_standard_deviation = torch.exp(0.5 * z_log_variance)\n",
    "    z = z_mean + z_standard_deviation * epsilon\n",
    "\n",
    "    return z\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Forward pass.\"\"\"\n",
    "\n",
    "    z_mean, z_log_variance = self.encode(x)\n",
    "    z = self.sample(z_mean, z_log_variance)\n",
    "    x = self.decode(z)\n",
    "\n",
    "    return x, z_mean, z_log_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c36e937b-8cf7-4208-9951-cb9c8d2b16fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T12:51:48.090035Z",
     "iopub.status.busy": "2023-11-08T12:51:48.089680Z",
     "iopub.status.idle": "2023-11-08T12:51:48.094425Z",
     "shell.execute_reply": "2023-11-08T12:51:48.093796Z",
     "shell.execute_reply.started": "2023-11-08T12:51:48.090009Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToPILImage()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c492ef32-24e1-40fe-9d84-14cd9c43cb10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T10:32:36.595367Z",
     "iopub.status.busy": "2023-11-08T10:32:36.594923Z",
     "iopub.status.idle": "2023-11-08T10:32:36.609634Z",
     "shell.execute_reply": "2023-11-08T10:32:36.608946Z",
     "shell.execute_reply.started": "2023-11-08T10:32:36.595335Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = VariationalAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "abb32842-e151-41a3-989a-c5a77849ffcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T10:32:37.145825Z",
     "iopub.status.busy": "2023-11-08T10:32:37.145448Z",
     "iopub.status.idle": "2023-11-08T10:32:37.156008Z",
     "shell.execute_reply": "2023-11-08T10:32:37.155400Z",
     "shell.execute_reply.started": "2023-11-08T10:32:37.145801Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64, 2, 2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode.cuda().encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "16347070-f07d-4071-8c22-4a8d0c814d03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T10:13:06.640172Z",
     "iopub.status.busy": "2023-11-08T10:13:06.639746Z",
     "iopub.status.idle": "2023-11-08T10:13:06.645222Z",
     "shell.execute_reply": "2023-11-08T10:13:06.644410Z",
     "shell.execute_reply.started": "2023-11-08T10:13:06.640142Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "\n",
    "def ELBOLoss(kl_weight: float = 1.0):\n",
    "  \"\"\"Computes the ELBO loss.\"\"\"\n",
    "\n",
    "  def loss(\n",
    "      xp: torch.Tensor,\n",
    "      x: torch.Tensor,\n",
    "      z_mean: torch.Tensor,\n",
    "      z_log_variance: torch.Tensor,\n",
    "  ) -> torch.Tensor:\n",
    "\n",
    "    reconstruction_loss = F.binary_cross_entropy(xp, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_log_variance - z_mean.pow(2) - z_log_variance.exp())\n",
    "\n",
    "    return reconstruction_loss + kl_weight * kl_loss\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c4f52c73-bb65-4643-9762-2ac3d25dc62b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T10:13:06.999984Z",
     "iopub.status.busy": "2023-11-08T10:13:06.999669Z",
     "iopub.status.idle": "2023-11-08T10:13:07.004083Z",
     "shell.execute_reply": "2023-11-08T10:13:07.003456Z",
     "shell.execute_reply.started": "2023-11-08T10:13:06.999963Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(latent_dimension: int, batch_size: int, scale: float = 0.2) -> torch.Tensor:\n",
    "\n",
    "    z = scale * torch.randn(batch_size, latent_dimension).cuda()\n",
    "    x = model.decode(z).detach()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "65801d50-152f-4459-9d4d-9bf0df8f9182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T10:13:07.588212Z",
     "iopub.status.busy": "2023-11-08T10:13:07.587826Z",
     "iopub.status.idle": "2023-11-08T10:13:07.621308Z",
     "shell.execute_reply": "2023-11-08T10:13:07.620688Z",
     "shell.execute_reply.started": "2023-11-08T10:13:07.588187Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "\n",
    "batch_size = 128\n",
    "image_channels = 3\n",
    "image_width = 128\n",
    "image_height = 128\n",
    "\n",
    "model = VariationalAutoencoder(in_channels=image_channels).cuda()\n",
    "criterion = ELBOLoss(kl_weight=50.)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 3\n",
    "batches = len(train_dataset) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "574c0cd9-c0a8-4860-9c31-eb0cfa8ddc7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T11:45:50.137300Z",
     "iopub.status.busy": "2023-11-08T11:45:50.136903Z",
     "iopub.status.idle": "2023-11-08T11:46:02.503249Z",
     "shell.execute_reply": "2023-11-08T11:46:02.502280Z",
     "shell.execute_reply.started": "2023-11-08T11:45:50.137273Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] epoch: 0/3, batch: 0019/1582, loss: \u001b[34m27020.8270\u001b[0m \u001b[31m(+100.00%)\u001b[0m\n",
      "[info] epoch: 0/3, batch: 0039/1582, loss: \u001b[34m26975.8664\u001b[0m \u001b[32m(-0.17%)\u001b[0m\n",
      "[info] epoch: 0/3, batch: 0059/1582, loss: \u001b[34m27024.0349\u001b[0m \u001b[31m(+0.18%)\u001b[0m\n",
      "[info] epoch: 0/3, batch: 0079/1582, loss: \u001b[34m27089.3137\u001b[0m \u001b[31m(+0.24%)\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 24\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m batch_size)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     28\u001b[0m     current_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(losses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(losses)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@markdown\n",
    "\n",
    "import chalk\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "model.train()\n",
    "\n",
    "previous_loss = 1.\n",
    "current_loss = 1.\n",
    "losses = []\n",
    "loss_normalizer = image_width * image_height * image_channels * batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        x = batch.cuda()\n",
    "        xp, z_mean, z_log_variance = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(xp, x, z_mean, z_log_variance)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item() / batch_size)\n",
    "\n",
    "        if (i + 1) % 20 == 0:\n",
    "\n",
    "            current_loss = sum(losses) / len(losses)\n",
    "            percent_change = ((current_loss - previous_loss) / current_loss) * 100\n",
    "            sign_percent_change = '' if percent_change < 0 else '+'\n",
    "            change_indicator = f'({sign_percent_change}{percent_change:0.2f}%)'\n",
    "            change_indicator = chalk.green(change_indicator) if percent_change < 0 else chalk.red(change_indicator)\n",
    "            loss_indicator = 'loss: ' + chalk.blue(f'{current_loss:0.4f}')\n",
    "            \n",
    "            previous_loss = current_loss\n",
    "            \n",
    "            print(f'[info] epoch: {epoch}/{epochs}, batch: {i:04d}/{batches}, {loss_indicator} {change_indicator}')\n",
    "            \n",
    "            losses.clear()\n",
    "        \n",
    "        if (i + 1) % 200 == 0:\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            torch.save(model, './model.checkpoint')\n",
    "            \n",
    "            save_image(xp[:64], './reconstructions.png')\n",
    "            save_image(x[:64], './inputs.png')\n",
    "            save_image(generate(latent_dimension=256, batch_size=64, scale=0.5), 'samples.png')\n",
    "            \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d6b189ed-5931-417c-8d7c-792aaaf8832a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T11:46:51.671419Z",
     "iopub.status.busy": "2023-11-08T11:46:51.670884Z",
     "iopub.status.idle": "2023-11-08T11:46:52.448866Z",
     "shell.execute_reply": "2023-11-08T11:46:52.448209Z",
     "shell.execute_reply.started": "2023-11-08T11:46:51.671378Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "save_image(generate(latent_dimension=256, batch_size=64, scale=1), 'samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938517a7-61ce-4393-b343-4bbffe069c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
